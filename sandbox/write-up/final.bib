
@article{margossian_review_2019,
	title = {A {Review} of automatic differentiation and its efficient implementation},
	volume = {9},
	issn = {1942-4787, 1942-4795},
	url = {http://arxiv.org/abs/1811.05031},
	doi = {10.1002/WIDM.1305},
	abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of automatic differentiation however requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open problems include the extension of current packages to provide more specialized routines, and efficient methods to perform higher-order differentiation.},
	number = {4},
	urldate = {2021-04-23},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Margossian, Charles C.},
	month = jul,
	year = {2019},
	note = {arXiv: 1811.05031},
	keywords = {Computer Science - Mathematical Software, Statistics - Computation},
	annote = {Comment: 32 pages, 5 figures, submitted for publication. WIREs Data Mining Knowl Discov, March 2019},
	file = {arXiv Fulltext PDF:/Users/quee/Zotero/storage/S7MAZ6S2/Margossian - 2019 - A Review of automatic differentiation and its effi.pdf:application/pdf;arXiv.org Snapshot:/Users/quee/Zotero/storage/XV88JES4/1811.html:text/html}
}

@article{kristensen_tmb_2016,
	title = {{TMB}: {Automatic} {Differentiation} and {Laplace} {Approximation}},
	volume = {70},
	issn = {1548-7660},
	shorttitle = {{TMB}},
	url = {http://arxiv.org/abs/1509.00660},
	doi = {10.18637/jss.v070.i05},
	abstract = {TMB is an open source R package that enables quick implementation of complex nonlinear random effect (latent variable) models in a manner similar to the established AD Model Builder package (ADMB, admb-project.org). In addition, it offers easy access to parallel computations. The user defines the joint likelihood for the data and the random effects as a C++ template function, while all the other operations are done in R; e.g., reading in the data. The package evaluates and maximizes the Laplace approximation of the marginal likelihood where the random effects are automatically integrated out. This approximation, and its derivatives, are obtained using automatic differentiation (up to order three) of the joint likelihood. The computations are designed to be fast for problems with many random effects ({\textasciitilde}10{\textasciicircum}6) and parameters ({\textasciitilde}10{\textasciicircum}3). Computation times using ADMB and TMB are compared on a suite of examples ranging from simple models to large spatial models where the random effects are a Gaussian random field. Speedups ranging from 1.5 to about 100 are obtained with increasing gains for large problems. The package and examples are available at http://tmb-project.org.},
	number = {5},
	urldate = {2021-04-23},
	journal = {Journal of Statistical Software},
	author = {Kristensen, Kasper and Nielsen, Anders and Berg, Casper W. and Skaug, Hans and Bell, Brad},
	year = {2016},
	note = {arXiv: 1509.00660},
	keywords = {Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/quee/Zotero/storage/JKIPRDL6/Kristensen et al. - 2016 - TMB Automatic Differentiation and Laplace Approxi.pdf:application/pdf;arXiv.org Snapshot:/Users/quee/Zotero/storage/USH8E4WW/1509.html:text/html}
}

@article{woltman_introduction_2012,
	title = {An introduction to hierarchical linear modeling},
	volume = {8},
	issn = {1913-4126},
	url = {http://www.tqmp.org/RegularArticles/vol08-1/p052},
	doi = {10.20982/tqmp.08.1.p052},
	language = {en},
	number = {1},
	urldate = {2021-04-23},
	journal = {Tutorials in Quantitative Methods for Psychology},
	author = {Woltman, Heather and Feldstain, Andrea and MacKay, J. Christine and Rocchi, Meredith},
	month = feb,
	year = {2012},
	pages = {52--69},
	file = {Woltman et al. - 2012 - An introduction to hierarchical linear modeling.pdf:/Users/quee/Zotero/storage/5WUXFF6Y/Woltman et al. - 2012 - An introduction to hierarchical linear modeling.pdf:application/pdf}
}

@article{griewank_introduction_2003,
	title = {Introduction to {Automatic} {Differentiation}},
	volume = {2},
	issn = {1617-7061},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pamm.200310012},
	doi = {https://doi.org/10.1002/pamm.200310012},
	abstract = {Automatic, or algorithmic, differentiation (AD) is a chain rule-based technique for evaluating derivatives of functions given as computer programs for their elimination. We review the main characteristics and application of AD and illustrate the methodology on a simple example.},
	language = {en},
	number = {1},
	urldate = {2021-04-23},
	journal = {PAMM},
	author = {Griewank, Andreas and Walther, Andrea},
	year = {2003},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pamm.200310012},
	pages = {45--49},
	file = {Snapshot:/Users/quee/Zotero/storage/6R4QATIY/pamm.html:text/html;Full Text PDF:/Users/quee/Zotero/storage/C3YSWDK2/Griewank and Walther - 2003 - Introduction to Automatic Differentiation.pdf:application/pdf}
}

@article{griewank_introduction_2003-1,
	title = {Introduction to {Automatic} {Differentiation}},
	volume = {2},
	issn = {1617-7061},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pamm.200310012},
	doi = {https://doi.org/10.1002/pamm.200310012},
	abstract = {Automatic, or algorithmic, differentiation (AD) is a chain rule-based technique for evaluating derivatives of functions given as computer programs for their elimination. We review the main characteristics and application of AD and illustrate the methodology on a simple example.},
	language = {en},
	number = {1},
	urldate = {2021-04-23},
	journal = {PAMM},
	author = {Griewank, Andreas and Walther, Andrea},
	year = {2003},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pamm.200310012},
	pages = {45--49},
	file = {Snapshot:/Users/quee/Zotero/storage/YALZCGDK/pamm.html:text/html;Full Text PDF:/Users/quee/Zotero/storage/SIS3LHNX/Griewank and Walther - 2003 - Introduction to Automatic Differentiation.pdf:application/pdf}
}

@article{laue_equivalence_2020,
	title = {On the {Equivalence} of {Forward} {Mode} {Automatic} {Differentiation} and {Symbolic} {Differentiation}},
	url = {http://arxiv.org/abs/1904.02990},
	abstract = {We show that forward mode automatic differentiation and symbolic differentiation are equivalent in the sense that they both perform the same operations when computing derivatives. This is in stark contrast to the common claim that they are substantially different. The difference is often illustrated by claiming that symbolic differentiation suffers from "expression swell" whereas automatic differentiation does not. Here, we show that this statement is not true. "Expression swell" refers to the phenomenon of a much larger representation of the derivative as opposed to the representation of the original function.},
	urldate = {2021-04-25},
	journal = {arXiv:1904.02990 [cs]},
	author = {Laue, Soeren},
	month = jul,
	year = {2020},
	note = {arXiv: 1904.02990},
	keywords = {Computer Science - Machine Learning, Computer Science - Symbolic Computation},
	file = {arXiv Fulltext PDF:/Users/quee/Zotero/storage/FDVE34QU/Laue - 2020 - On the Equivalence of Forward Mode Automatic Diffe.pdf:application/pdf;arXiv.org Snapshot:/Users/quee/Zotero/storage/5KB3L4G7/1904.html:text/html}
}

@article{larson_derivative-free_2019,
	title = {Derivative-free optimization methods},
	volume = {28},
	issn = {0962-4929, 1474-0508},
	url = {http://arxiv.org/abs/1904.11585},
	doi = {10.1017/S0962492919000060},
	abstract = {In many optimization problems arising from scientific, engineering and artificial intelligence applications, objective and constraint functions are available only as the output of a black-box or simulation oracle that does not provide derivative information. Such settings necessitate the use of methods for derivative-free, or zeroth-order, optimization. We provide a review and perspectives on developments in these methods, with an emphasis on highlighting recent developments and on unifying treatment of such problems in the non-linear optimization and machine learning literature. We categorize methods based on assumed properties of the black-box functions, as well as features of the methods. We first overview the primary setting of deterministic methods applied to unconstrained, non-convex optimization problems where the objective function is defined by a deterministic black-box oracle. We then discuss developments in randomized methods, methods that assume some additional structure about the objective (including convexity, separability and general non-smooth compositions), methods for problems where the output of the black-box oracle is stochastic, and methods for handling different types of constraints.},
	urldate = {2021-04-26},
	journal = {Acta Numerica},
	author = {Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M.},
	month = may,
	year = {2019},
	note = {arXiv: 1904.11585},
	keywords = {Mathematics - Optimization and Control},
	pages = {287--404},
	file = {arXiv Fulltext PDF:/Users/quee/Zotero/storage/UY7EKFBL/Larson et al. - 2019 - Derivative-free optimization methods.pdf:application/pdf;arXiv.org Snapshot:/Users/quee/Zotero/storage/5A2SXUZ9/1904.html:text/html}
}

@article{dai_convergence_2002,
	title = {Convergence {Properties} of the {BFGS} {Algoritm}},
	volume = {13},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/abs/10.1137/S1052623401383455},
	doi = {10.1137/S1052623401383455},
	abstract = {The BFGS method is one of the most famous quasi-Newton algorithms for unconstrained optimization. In 1984, Powell presented an example of a function of two variables that shows that the Polak--Ribière--Polyak (PRP) conjugate gradient method and the BFGS quasi-Newton method may cycle around eight nonstationary points if each line search picks a local minimum that provides a reduction in the objective function. In this paper, a new technique of choosing parameters is introduced, and an example with only six cyclic points is provided. It is also noted through the examples that the BFGS method with Wolfe line searches need not converge for nonconvex objective functions.},
	number = {3},
	urldate = {2021-04-26},
	journal = {SIAM Journal on Optimization},
	author = {Dai, Yu-Hong},
	month = jan,
	year = {2002},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {693--701},
	file = {Snapshot:/Users/quee/Zotero/storage/IBBTTR64/S1052623401383455.html:text/html;Dai - 2002 - Convergence Properties of the BFGS Algoritm.pdf:/Users/quee/Zotero/storage/SMM8398Z/Dai - 2002 - Convergence Properties of the BFGS Algoritm.pdf:application/pdf}
}

@article{schoenberg_optimization_nodate,
	title = {Optimization with the {Quasi}-{Newton} {Method}},
	language = {en},
	author = {Schoenberg, Ronald},
	pages = {13},
	file = {Schoenberg - Optimization with the Quasi-Newton Method.pdf:/Users/quee/Zotero/storage/QA2KC8MU/Schoenberg - Optimization with the Quasi-Newton Method.pdf:application/pdf}
}

@article{ben-israel_newton-raphson_1966,
	title = {A {Newton}-{Raphson} method for the solution of systems of equations},
	volume = {15},
	issn = {0022-247X},
	url = {https://www.sciencedirect.com/science/article/pii/0022247X66901156},
	doi = {10.1016/0022-247X(66)90115-6},
	language = {en},
	number = {2},
	urldate = {2021-04-26},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Ben-Israel, Adi},
	month = aug,
	year = {1966},
	pages = {243--252},
	file = {ScienceDirect Full Text PDF:/Users/quee/Zotero/storage/574GCTHD/Ben-Israel - 1966 - A Newton-Raphson method for the solution of system.pdf:application/pdf;ScienceDirect Snapshot:/Users/quee/Zotero/storage/HB5CPGGQ/0022247X66901156.html:text/html}
}

@article{shanno_conditioning_1970,
	title = {Conditioning of {Quasi}-{Newton} {Methods} for {Function} {Minimization}},
	volume = {24},
	issn = {0025-5718},
	url = {https://www.jstor.org/stable/2004840},
	doi = {10.2307/2004840},
	abstract = {Quasi-Newton methods accelerate the steepest-descent technique for function minimization by using computational history to generate a sequence of approximations to the inverse of the Hessian matrix. This paper presents a class of approximating matrices as a function of a scalar parameter. The problem of optimal conditioning of these matrices under an appropriate norm as a function of the scalar parameter is investigated. A set of computational results verifies the superiority of the new methods arising from conditioning considerations to known methods.},
	number = {111},
	urldate = {2021-04-26},
	journal = {Mathematics of Computation},
	author = {Shanno, D. F.},
	year = {1970},
	note = {Publisher: American Mathematical Society},
	pages = {647--656},
	file = {Full Text:/Users/quee/Zotero/storage/4PGGCBCS/Shanno - 1970 - Conditioning of Quasi-Newton Methods for Function .pdf:application/pdf}
}

@article{durrbaum_comparison_2002,
	title = {Comparison of {Automatic} and {Symbolic} {Differentiation} in {Mathematical} {Modeling} and {Computer} {Simulation} of {Rigid}-{Body} {Systems}},
	volume = {7},
	issn = {1573-272X},
	url = {https://doi.org/10.1023/A:1015523018029},
	doi = {10.1023/A:1015523018029},
	abstract = {The objective of this paper is to check the efficiency and validity oftwo approaches for computing derivatives of complex functions,automatic differentiation using ADOLC and symbolicdifferentiation using MACSYMA. This has been done in three benchmarkexamples, where the gradient of a Helmholtz energy function has beencomputed for different dimensions of independent variables (Example 1)and Jacobian matrices of inverse kinematics of planar and spatialparallel robots (Examples 2 and 3) have been computed. The results havebeen evaluated under six criteria: preliminary implementation work,computation time, flexibility in applications, limits of applicability,accuracy, and memory requirements.},
	language = {en},
	number = {4},
	urldate = {2021-04-27},
	journal = {Multibody System Dynamics},
	author = {Dürrbaum, Axel and Klier, Willy and Hahn, Hubert},
	month = may,
	year = {2002},
	pages = {331--355},
	file = {Springer Full Text PDF:/Users/quee/Zotero/storage/JEWE7QMY/Dürrbaum et al. - 2002 - Comparison of Automatic and Symbolic Differentiati.pdf:application/pdf}
}

@article{stepleman_adaptive_1979,
	title = {Adaptive numerical differentiation},
	volume = {33},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/1979-33-148/S0025-5718-1979-0537969-8/},
	doi = {10.1090/S0025-5718-1979-0537969-8},
	abstract = {It is well known that the calculation of an accurate approximate derivative of a nontabular function on a finite-precision computer by the formula is a delicate task. If h is too large, truncation errors cause poor answers, while if h is too small, cancellation and other "rounding" errors cause poor answers. We will show that by using simple results on the nature of the asymptotic convergence of to , a reliable numerical method can be obtained which can yield efficiently the theoretical maximum number of accurate digits for the given machine precision.},
	language = {en},
	number = {148},
	urldate = {2021-04-27},
	journal = {Mathematics of Computation},
	author = {Stepleman, R. S. and Winarsky, N. D.},
	year = {1979},
	pages = {1257--1264},
	file = {Full Text PDF:/Users/quee/Zotero/storage/PDZG529S/Stepleman and Winarsky - 1979 - Adaptive numerical differentiation.pdf:application/pdf;Snapshot:/Users/quee/Zotero/storage/GA34EZVJ/S0025-5718-1979-0537969-8.html:text/html}
}
