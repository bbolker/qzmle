
@article{margossian_review_2019,
	title = {A Review of automatic differentiation and its efficient implementation},
	volume = {9},
	issn = {1942-4787, 1942-4795},
	url = {http://arxiv.org/abs/1811.05031},
	doi = {10.1002/WIDM.1305},
	abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of automatic differentiation however requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open problems include the extension of current packages to provide more specialized routines, and efficient methods to perform higher-order differentiation.},
	number = {4},
	journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
	shortjournal = {{WIREs} Data Mining Knowl Discov},
	author = {Margossian, Charles C.},
	urldate = {2021-04-23},
	date = {2019-07},
	eprinttype = {arxiv},
	eprint = {1811.05031},
	keywords = {Computer Science - Mathematical Software, Statistics - Computation},
	annotation = {Comment: 32 pages, 5 figures, submitted for publication. {WIREs} Data Mining Knowl Discov, March 2019},
	file = {arXiv Fulltext PDF:/Users/quee/Zotero/storage/S7MAZ6S2/Margossian - 2019 - A Review of automatic differentiation and its effi.pdf:application/pdf;arXiv.org Snapshot:/Users/quee/Zotero/storage/XV88JES4/1811.html:text/html}
}

@article{kristensen_tmb_2016,
	title = {{TMB}: Automatic Differentiation and Laplace Approximation},
	volume = {70},
	issn = {1548-7660},
	url = {http://arxiv.org/abs/1509.00660},
	doi = {10.18637/jss.v070.i05},
	shorttitle = {{TMB}},
	abstract = {{TMB} is an open source R package that enables quick implementation of complex nonlinear random effect (latent variable) models in a manner similar to the established {AD} Model Builder package ({ADMB}, admb-project.org). In addition, it offers easy access to parallel computations. The user defines the joint likelihood for the data and the random effects as a C++ template function, while all the other operations are done in R; e.g., reading in the data. The package evaluates and maximizes the Laplace approximation of the marginal likelihood where the random effects are automatically integrated out. This approximation, and its derivatives, are obtained using automatic differentiation (up to order three) of the joint likelihood. The computations are designed to be fast for problems with many random effects ({\textasciitilde}10{\textasciicircum}6) and parameters ({\textasciitilde}10{\textasciicircum}3). Computation times using {ADMB} and {TMB} are compared on a suite of examples ranging from simple models to large spatial models where the random effects are a Gaussian random field. Speedups ranging from 1.5 to about 100 are obtained with increasing gains for large problems. The package and examples are available at http://tmb-project.org.},
	number = {5},
	journaltitle = {Journal of Statistical Software},
	shortjournal = {J. Stat. Soft.},
	author = {Kristensen, Kasper and Nielsen, Anders and Berg, Casper W. and Skaug, Hans and Bell, Brad},
	urldate = {2021-04-23},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1509.00660},
	keywords = {Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/quee/Zotero/storage/JKIPRDL6/Kristensen et al. - 2016 - TMB Automatic Differentiation and Laplace Approxi.pdf:application/pdf;arXiv.org Snapshot:/Users/quee/Zotero/storage/USH8E4WW/1509.html:text/html}
}

@article{woltman_introduction_2012,
	title = {An introduction to hierarchical linear modeling},
	volume = {8},
	issn = {1913-4126},
	url = {http://www.tqmp.org/RegularArticles/vol08-1/p052},
	doi = {10.20982/tqmp.08.1.p052},
	pages = {52--69},
	number = {1},
	journaltitle = {Tutorials in Quantitative Methods for Psychology},
	shortjournal = {{TQMP}},
	author = {Woltman, Heather and Feldstain, Andrea and {MacKay}, J. Christine and Rocchi, Meredith},
	urldate = {2021-04-23},
	date = {2012-02-01},
	langid = {english},
	file = {Woltman et al. - 2012 - An introduction to hierarchical linear modeling.pdf:/Users/quee/Zotero/storage/5WUXFF6Y/Woltman et al. - 2012 - An introduction to hierarchical linear modeling.pdf:application/pdf}
}

@article{griewank_introduction_2003,
	title = {Introduction to Automatic Differentiation},
	volume = {2},
	issn = {1617-7061},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pamm.200310012},
	doi = {https://doi.org/10.1002/pamm.200310012},
	abstract = {Automatic, or algorithmic, differentiation ({AD}) is a chain rule-based technique for evaluating derivatives of functions given as computer programs for their elimination. We review the main characteristics and application of {AD} and illustrate the methodology on a simple example.},
	pages = {45--49},
	number = {1},
	journaltitle = {{PAMM}},
	author = {Griewank, Andreas and Walther, Andrea},
	urldate = {2021-04-23},
	date = {2003},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pamm.200310012},
	file = {Snapshot:/Users/quee/Zotero/storage/YALZCGDK/pamm.html:text/html;Full Text PDF:/Users/quee/Zotero/storage/SIS3LHNX/Griewank and Walther - 2003 - Introduction to Automatic Differentiation.pdf:application/pdf}
}

@article{laue_equivalence_2020,
	title = {On the Equivalence of Forward Mode Automatic Differentiation and Symbolic Differentiation},
	url = {http://arxiv.org/abs/1904.02990},
	abstract = {We show that forward mode automatic differentiation and symbolic differentiation are equivalent in the sense that they both perform the same operations when computing derivatives. This is in stark contrast to the common claim that they are substantially different. The difference is often illustrated by claiming that symbolic differentiation suffers from "expression swell" whereas automatic differentiation does not. Here, we show that this statement is not true. "Expression swell" refers to the phenomenon of a much larger representation of the derivative as opposed to the representation of the original function.},
	journaltitle = {{arXiv}:1904.02990 [cs]},
	author = {Laue, Soeren},
	urldate = {2021-04-25},
	date = {2020-07-20},
	eprinttype = {arxiv},
	eprint = {1904.02990},
	keywords = {Computer Science - Machine Learning, Computer Science - Symbolic Computation},
	file = {arXiv Fulltext PDF:/Users/quee/Zotero/storage/FDVE34QU/Laue - 2020 - On the Equivalence of Forward Mode Automatic Diffe.pdf:application/pdf;arXiv.org Snapshot:/Users/quee/Zotero/storage/5KB3L4G7/1904.html:text/html}
}

@article{larson_derivative-free_2019,
	title = {Derivative-free optimization methods},
	volume = {28},
	issn = {0962-4929, 1474-0508},
	url = {http://arxiv.org/abs/1904.11585},
	doi = {10.1017/S0962492919000060},
	abstract = {In many optimization problems arising from scientific, engineering and artificial intelligence applications, objective and constraint functions are available only as the output of a black-box or simulation oracle that does not provide derivative information. Such settings necessitate the use of methods for derivative-free, or zeroth-order, optimization. We provide a review and perspectives on developments in these methods, with an emphasis on highlighting recent developments and on unifying treatment of such problems in the non-linear optimization and machine learning literature. We categorize methods based on assumed properties of the black-box functions, as well as features of the methods. We first overview the primary setting of deterministic methods applied to unconstrained, non-convex optimization problems where the objective function is defined by a deterministic black-box oracle. We then discuss developments in randomized methods, methods that assume some additional structure about the objective (including convexity, separability and general non-smooth compositions), methods for problems where the output of the black-box oracle is stochastic, and methods for handling different types of constraints.},
	pages = {287--404},
	journaltitle = {Acta Numerica},
	shortjournal = {Acta Numerica},
	author = {Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M.},
	urldate = {2021-04-26},
	date = {2019-05-01},
	eprinttype = {arxiv},
	eprint = {1904.11585},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/quee/Zotero/storage/UY7EKFBL/Larson et al. - 2019 - Derivative-free optimization methods.pdf:application/pdf;arXiv.org Snapshot:/Users/quee/Zotero/storage/5A2SXUZ9/1904.html:text/html}
}

@article{dai_convergence_2002,
	title = {Convergence Properties of the {BFGS} Algoritm},
	volume = {13},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/abs/10.1137/S1052623401383455},
	doi = {10.1137/S1052623401383455},
	abstract = {The {BFGS} method is one of the most famous quasi-Newton algorithms for unconstrained optimization. In 1984, Powell presented an example of a function of two variables that shows that the Polak--Ribi√®re--Polyak ({PRP}) conjugate gradient method and the {BFGS} quasi-Newton method may cycle around eight nonstationary points if each line search picks a local minimum that provides a reduction in the objective function. In this paper, a new technique of choosing parameters is introduced, and an example with only six cyclic points is provided. It is also noted through the examples that the {BFGS} method with Wolfe line searches need not converge for nonconvex objective functions.},
	pages = {693--701},
	number = {3},
	journaltitle = {{SIAM} Journal on Optimization},
	shortjournal = {{SIAM} J. Optim.},
	author = {Dai, Yu-Hong},
	urldate = {2021-04-26},
	date = {2002-01-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Snapshot:/Users/quee/Zotero/storage/IBBTTR64/S1052623401383455.html:text/html;Dai - 2002 - Convergence Properties of the BFGS Algoritm.pdf:/Users/quee/Zotero/storage/SMM8398Z/Dai - 2002 - Convergence Properties of the BFGS Algoritm.pdf:application/pdf}
}

@article{schoenberg_optimization_nodate,
	title = {Optimization with the Quasi-Newton Method},
	pages = {13},
	author = {Schoenberg, Ronald},
	langid = {english},
	file = {Schoenberg - Optimization with the Quasi-Newton Method.pdf:/Users/quee/Zotero/storage/QA2KC8MU/Schoenberg - Optimization with the Quasi-Newton Method.pdf:application/pdf}
}

@article{ben-israel_newton-raphson_1966,
	title = {A Newton-Raphson method for the solution of systems of equations},
	volume = {15},
	issn = {0022-247X},
	url = {https://www.sciencedirect.com/science/article/pii/0022247X66901156},
	doi = {10.1016/0022-247X(66)90115-6},
	pages = {243--252},
	number = {2},
	journaltitle = {Journal of Mathematical Analysis and Applications},
	shortjournal = {Journal of Mathematical Analysis and Applications},
	author = {Ben-Israel, Adi},
	urldate = {2021-04-26},
	date = {1966-08-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/quee/Zotero/storage/574GCTHD/Ben-Israel - 1966 - A Newton-Raphson method for the solution of system.pdf:application/pdf;ScienceDirect Snapshot:/Users/quee/Zotero/storage/HB5CPGGQ/0022247X66901156.html:text/html}
}

@article{shanno_conditioning_1970,
	title = {Conditioning of Quasi-Newton Methods for Function Minimization},
	volume = {24},
	issn = {0025-5718},
	url = {https://www.jstor.org/stable/2004840},
	doi = {10.2307/2004840},
	abstract = {Quasi-Newton methods accelerate the steepest-descent technique for function minimization by using computational history to generate a sequence of approximations to the inverse of the Hessian matrix. This paper presents a class of approximating matrices as a function of a scalar parameter. The problem of optimal conditioning of these matrices under an appropriate norm as a function of the scalar parameter is investigated. A set of computational results verifies the superiority of the new methods arising from conditioning considerations to known methods.},
	pages = {647--656},
	number = {111},
	journaltitle = {Mathematics of Computation},
	author = {Shanno, D. F.},
	urldate = {2021-04-26},
	date = {1970},
	note = {Publisher: American Mathematical Society},
	file = {Full Text:/Users/quee/Zotero/storage/4PGGCBCS/Shanno - 1970 - Conditioning of Quasi-Newton Methods for Function .pdf:application/pdf}
}

@article{durrbaum_comparison_2002,
	title = {Comparison of Automatic and Symbolic Differentiation in Mathematical Modeling and Computer Simulation of Rigid-Body Systems},
	volume = {7},
	issn = {1573-272X},
	url = {https://doi.org/10.1023/A:1015523018029},
	doi = {10.1023/A:1015523018029},
	abstract = {The objective of this paper is to check the efficiency and validity oftwo approaches for computing derivatives of complex functions,automatic differentiation using {ADOLC} and symbolicdifferentiation using {MACSYMA}. This has been done in three benchmarkexamples, where the gradient of a Helmholtz energy function has beencomputed for different dimensions of independent variables (Example 1)and Jacobian matrices of inverse kinematics of planar and spatialparallel robots (Examples 2 and 3) have been computed. The results havebeen evaluated under six criteria: preliminary implementation work,computation time, flexibility in applications, limits of applicability,accuracy, and memory requirements.},
	pages = {331--355},
	number = {4},
	journaltitle = {Multibody System Dynamics},
	shortjournal = {Multibody System Dynamics},
	author = {D√ºrrbaum, Axel and Klier, Willy and Hahn, Hubert},
	urldate = {2021-04-27},
	date = {2002-05-01},
	langid = {english},
	file = {Springer Full Text PDF:/Users/quee/Zotero/storage/JEWE7QMY/D√ºrrbaum et al. - 2002 - Comparison of Automatic and Symbolic Differentiati.pdf:application/pdf}
}

@article{stepleman_adaptive_1979,
	title = {Adaptive numerical differentiation},
	volume = {33},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/1979-33-148/S0025-5718-1979-0537969-8/},
	doi = {10.1090/S0025-5718-1979-0537969-8},
	abstract = {It is well known that the calculation of an accurate approximate derivative of a nontabular function on a finite-precision computer by the formula is a delicate task. If h is too large, truncation errors cause poor answers, while if h is too small, cancellation and other "rounding" errors cause poor answers. We will show that by using simple results on the nature of the asymptotic convergence of to , a reliable numerical method can be obtained which can yield efficiently the theoretical maximum number of accurate digits for the given machine precision.},
	pages = {1257--1264},
	number = {148},
	journaltitle = {Mathematics of Computation},
	shortjournal = {Math. Comp.},
	author = {Stepleman, R. S. and Winarsky, N. D.},
	urldate = {2021-04-27},
	date = {1979},
	langid = {english},
	file = {Full Text PDF:/Users/quee/Zotero/storage/PDZG529S/Stepleman and Winarsky - 1979 - Adaptive numerical differentiation.pdf:application/pdf;Snapshot:/Users/quee/Zotero/storage/GA34EZVJ/S0025-5718-1979-0537969-8.html:text/html}
}

@book{bolker_ecological_2008,
	title = {Ecological Models and Data in R},
	isbn = {978-0-691-12522-0},
	abstract = {{\textless}em{\textgreater}Ecological Models and Data in R is the first truly practical introduction to modern statistical methods for ecology. In step-by-step detail, the book teaches ecology graduate students and researchers everything they need to know in order to use maximum likelihood, information-theoretic, and Bayesian techniques to analyze their own data using the programming language R. Drawing on extensive experience teaching these techniques to graduate students in ecology, Benjamin Bolker shows how to choose among and construct statistical models for data, estimate their parameters and confidence limits, and interpret the results. The book also covers statistical frameworks, the philosophy of statistical modeling, and critical mathematical functions and probability distributions. It requires no programming background--only basic calculus and statistics.   {\textless}ul{\textgreater} {\textless}li{\textgreater}{\textless}br{\textgreater} {\textless}li{\textgreater}Practical, beginner-friendly introduction to modern statistical techniques for ecology using the programming language R {\textless}li{\textgreater}{\textless}br{\textgreater} {\textless}li{\textgreater}Step-by-step instructions for fitting models to messy, real-world data {\textless}li{\textgreater}{\textless}br{\textgreater} {\textless}li{\textgreater}Balanced view of different statistical approaches {\textless}li{\textgreater}{\textless}br{\textgreater} {\textless}li{\textgreater}Wide coverage of techniques--from simple (distribution fitting) to complex (state-space modeling) {\textless}li{\textgreater}{\textless}br{\textgreater} {\textless}li{\textgreater}Techniques for data manipulation and graphical display {\textless}li{\textgreater}{\textless}br{\textgreater} {\textless}li{\textgreater}Companion Web site with data and R code for all examples {\textless}li{\textgreater}{\textless}br{\textgreater} {\textless}br{\textgreater} {\textless}br{\textgreater}},
	publisher = {Princeton University Press},
	author = {Bolker, Benjamin M.},
	urldate = {2021-04-28},
	date = {2008},
	doi = {10.2307/j.ctvcm4g37}
}

@article{vonesh_egg_2005,
	title = {Egg predation and predator-induced hatching plasticity in the African reed frog, Hyperolius spinigularis},
	volume = {110},
	issn = {1600-0706},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0030-1299.2005.13759.x},
	doi = {https://doi.org/10.1111/j.0030-1299.2005.13759.x},
	abstract = {Predators can reduce prey density through consumption and induce changes in the phenotypes of surviving prey (e.g. their behavior, morphology, life history). Recent reviews have highlighted the importance of both types of effects for understanding species interactions in food webs. However, most studies focus on only density or trait effects-few have examined both within a single predator‚Äìprey system. Here I examine both the density and trait effects of egg-stage predators of the East African reed frog, Hyperolius spinigularis, a species with arboreal clutches and aquatic larvae. To quantify the density effects of egg predators, I monitored seasonal clutch production and survivorship over two breeding seasons and thus estimated the effects of different predators on larval input into the aquatic habitat. I also quantified the effects of egg predators on the timing of hatching and hatchling phenotype. The reed frog, Afrixalus fornasini, and the ephydrid fly, Typopsilopa sp., caused most of the mortality of H. spinigularis eggs, so I focused on these two predators. Egg predation reduced larval densities from 18 to 82\% depending upon the time of the season. Predation by Afrixalus and Typopsilopa caused surviving embryos to hatch 3‚Äì4 days earlier, at an earlier developmental stage, and at sizes that were 20‚Äì30\% smaller compared to hatchlings from undisturbed clutches. Recent experimental studies in this system have demonstrated that reductions in larval density and size due to egg-stage predators facilitate larval survival in the presence of aquatic predators. Reductions in larval density increase larval survival by reducing larval competition and increasing growth through vulnerable size classes. Predator-induced early-hatched tadpoles exhibited higher larval growth rates than tadpoles from undisturbed clutches, and thus realized higher survival by growing more rapidly though vulnerable size classes. Thus both the density and trait effects of egg-stage predators have important, sometimes unexpected, consequences for survival in subsequent life-stages.},
	pages = {241--252},
	number = {2},
	journaltitle = {Oikos},
	author = {Vonesh, James R.},
	urldate = {2021-04-28},
	date = {2005},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0030-1299.2005.13759.x}
}

@online{bell_cppad_2005,
	title = {{CppAD}: A Package for C++ Algorithmic Differentiation},
	url = {http: //www.coin-or.org/CppAD},
	author = {Bell, {BM}},
	urldate = {2021-04-28},
	date = {2005},
	file = {cppad-20210426\: A C++ Algorithmic Differentiation Package:/Users/quee/Zotero/storage/5GZZ8MMF/cppad.html:text/html}
}

@book{pinheiro_mixed-effects_2000,
	location = {New York},
	title = {Mixed-Effects Models in S and S-{PLUS}},
	isbn = {978-0-387-98957-0},
	url = {https://www.springer.com/gp/book/9780387989570},
	series = {Statistics and Computing},
	abstract = {Mixed-Effects Models in S and S-{PLUS}...},
	publisher = {Springer-Verlag},
	author = {Pinheiro, Jos√© and Bates, Douglas},
	urldate = {2021-04-28},
	date = {2000},
	langid = {english},
	doi = {10.1007/b98882},
	file = {Snapshot:/Users/quee/Zotero/storage/2AN3SLZH/9780387989570.html:text/html}
}

@article{hogan_fast_2014,
	title = {Fast Reverse-Mode Automatic Differentiation using Expression Templates in C++},
	volume = {40},
	issn = {0098-3500},
	url = {https://doi.org/10.1145/2560359},
	doi = {10.1145/2560359},
	abstract = {Gradient-based optimization problems are encountered in many fields, but the associated task of differentiating large computer algorithms can be formidable. The operator-overloading approach to performing reverse-mode automatic differentiation is the most convenient for the user but current implementations are typically 10-35 times slower than the original algorithm. In this paper a fast new operator-overloading method is presented that uses the expression template programming technique in C++ to provide a compile-time representation of each mathematical expression as a computational graph that can be efficiently traversed in either direction. Benchmarking with four different numerical algorithms shows this approach to be 2.6--9 times faster than current operator-overloading libraries, and 1.3--7.7 times more efficient in memory usage. It is typically less than 4 times the computational cost of the original algorithm, although poorer performance is found for all libraries in the case of simple loops containing no mathematical functions. An implementation is freely available in the Adept C++ software library.},
	pages = {26:1--26:16},
	number = {4},
	journaltitle = {{ACM} Transactions on Mathematical Software},
	shortjournal = {{ACM} Trans. Math. Softw.},
	author = {Hogan, Robin J.},
	urldate = {2021-04-28},
	date = {2014-07-08},
	keywords = {Adjoint code, Jacobian matrix, quasi-Newton, template metaprogramming}
}

@book{fitzmaurice_applied_2012,
	title = {Applied Longitudinal Analysis},
	isbn = {978-1-118-55179-0},
	abstract = {Praise for the First Edition ". . . [this book] should be on the shelf of everyone interested in . . . longitudinal data analysis." ‚ÄîJournal of the American Statistical Association Features newly developed topics and applications of the analysis of longitudinal data Applied Longitudinal Analysis, Second Edition presents modern methods for analyzing data from longitudinal studies and now features the latest state-of-the-art techniques. The book emphasizes practical, rather than theoretical, aspects of methods for the analysis of diverse types of longitudinal data that can be applied across various fields of study, from the health and medical sciences to the social and behavioral sciences. The authors incorporate their extensive academic and research experience along with various updates that have been made in response to reader feedback. The Second Edition features six newly added chapters that explore topics currently evolving in the field, including:  Fixed effects and mixed effects models Marginal models and generalized estimating equations Approximate methods for generalized linear mixed effects models Multiple imputation and inverse probability weighted methods Smoothing methods for longitudinal data Sample size and power  Each chapter presents methods in the setting of applications to data sets drawn from the health sciences. New problem sets have been added to many chapters, and a related website features sample programs and computer output using {SAS}, Stata, and R, as well as data sets and supplemental slides to facilitate a complete understanding of the material. With its strong emphasis on multidisciplinary applications and the interpretation of results, Applied Longitudinal Analysis, Second Edition is an excellent book for courses on statistics in the health and medical sciences at the upper-undergraduate and graduate levels. The book also serves as a valuable reference for researchers and professionals in the medical, public health, and pharmaceutical fields as well as those in social and behavioral sciences who would like to learn more about analyzing longitudinal data.},
	pagetotal = {742},
	publisher = {John Wiley \& Sons},
	author = {Fitzmaurice, Garrett M. and Laird, Nan M. and Ware, James H.},
	date = {2012-10-23},
	langid = {english},
	note = {Google-Books-{ID}: 0exUN1yFBHEC},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Multivariate Analysis, Mathematics / Probability \& Statistics / Stochastic Processes, Medical / Biostatistics}
}